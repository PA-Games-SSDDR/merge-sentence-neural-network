{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4632840,"sourceType":"datasetVersion","datasetId":2695334}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-03T17:34:32.140384Z","iopub.execute_input":"2023-12-03T17:34:32.140675Z","iopub.status.idle":"2023-12-03T17:34:32.487348Z","shell.execute_reply.started":"2023-12-03T17:34:32.140647Z","shell.execute_reply":"2023-12-03T17:34:32.486578Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Lambda, RepeatVector, TimeDistributed\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras as keras\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import pad_sequences\nimport string","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:34:32.488822Z","iopub.execute_input":"2023-12-03T17:34:32.489188Z","iopub.status.idle":"2023-12-03T17:34:44.016790Z","shell.execute_reply.started":"2023-12-03T17:34:32.489163Z","shell.execute_reply":"2023-12-03T17:34:44.015805Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Process and load the dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/unveiling-complex-text-relations-through-splitti/train.csv\")\n# test_df = pd.read_csv(\"/kaggle/input/unveiling-complex-text-relations-through-splitti/test.csv\")\n# cleanse data of punctuation\ndf['complex_sentence'] = df['complex_sentence'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\ndf['simple_sentence_1'] = df['simple_sentence_1'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\ndf['simple_sentence_2'] = df['simple_sentence_2'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:34:44.018283Z","iopub.execute_input":"2023-12-03T17:34:44.019085Z","iopub.status.idle":"2023-12-03T17:36:01.980138Z","shell.execute_reply.started":"2023-12-03T17:34:44.019057Z","shell.execute_reply":"2023-12-03T17:36:01.979229Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"complex_list = df['complex_sentence'].values.tolist()\nsimpl_sent1_list = df['simple_sentence_1'].values.tolist()\nsimpl_sent2_list = df['simple_sentence_2'].values.tolist()\nmax_len_complex = max([len(x.split(\" \")) for x in complex_list])\nmax_len_simpl_sent1 = max([len(x.split(\" \")) for x in simpl_sent1_list])\nmax_len_simpl_sent2 = max([len(x.split(\" \")) for x in simpl_sent2_list])\nmax_tok_size = max(max_len_complex, max(max_len_simpl_sent1, max_len_simpl_sent2))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:36:01.982144Z","iopub.execute_input":"2023-12-03T17:36:01.982431Z","iopub.status.idle":"2023-12-03T17:36:06.373939Z","shell.execute_reply.started":"2023-12-03T17:36:01.982406Z","shell.execute_reply":"2023-12-03T17:36:06.372914Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"new_list = [complex_list[i] + \" \" + simpl_sent1_list[i] + \" \" + simpl_sent2_list[i] for i in range(len(complex_list))]\nfit_text = new_list\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(fit_text)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:36:06.375173Z","iopub.execute_input":"2023-12-03T17:36:06.375477Z","iopub.status.idle":"2023-12-03T17:37:03.750599Z","shell.execute_reply.started":"2023-12-03T17:36:06.375452Z","shell.execute_reply":"2023-12-03T17:37:03.749794Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Define the Siamese neural network architecture\ndef create_siamese_network(max_sequence_length, embedding_dim, tokenizer_len):\n    # Input layer for the first sentence\n    input_a = Input(shape=(max_sequence_length,1), name='input_a')\n    \n    # Input layer for the second sentence\n    input_b = Input(shape=(max_sequence_length,1), name='input_b')\n    \n    # Shared embedding layer\n    vocabulary_size = tokenizer_len\n    # embedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\n    \n    # Shared LSTM layer\n    # lstm_layer = LSTM(128)\n    \n    # Apply embedding layer to both inputs\n    # encoded_a = embedding_layer(input_a)\n    # encoded_b = embedding_layer(input_b)\n    \n    # Encoder part\n    encoder_a = LSTM(64, activation='relu')(input_a)\n    encoder_b = LSTM(64, activation='relu')(input_b)\n    \n    # Merge the two encoded representations using a distance function (e.g., Euclidean or Manhattan)\n    # merged_layer = tf.keras.layers.Lambda(lambda x: tf.keras.backend.abs(x[0] - x[1]))([encoded_a, encoded_b])\n    merged_layer = Concatenate(axis=1, name='encoder_ab_output')([encoder_a,encoder_b])\n    \n    # Decoder part\n    decoder1 = RepeatVector(max_sequence_length)(merged_layer)\n    decoder1 = LSTM(max_sequence_length, activation='relu', return_sequences=True)(decoder1)\n    output_layer = TimeDistributed(Dense(1))(decoder1)\n    # Dense layer for the final similarity prediction\n    # output_layer = Dense(1, activation='sigmoid')(merged_layer)\n    \n    # Create the Siamese style autoencoder model\n    siamese_model = Model(inputs=[input_a, input_b], outputs=output_layer)\n    \n    siamese_model.compile(loss='mse', optimizer='adam')\n    print(siamese_model.summary())\n    return siamese_model","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:37:03.751705Z","iopub.execute_input":"2023-12-03T17:37:03.751990Z","iopub.status.idle":"2023-12-03T17:37:03.760548Z","shell.execute_reply.started":"2023-12-03T17:37:03.751965Z","shell.execute_reply":"2023-12-03T17:37:03.759586Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"siamese_model = create_siamese_network(max_tok_size, embedding_dim=100, tokenizer_len=len(tokenizer.word_index))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:37:03.761666Z","iopub.execute_input":"2023-12-03T17:37:03.761906Z","iopub.status.idle":"2023-12-03T17:37:06.984367Z","shell.execute_reply.started":"2023-12-03T17:37:03.761885Z","shell.execute_reply":"2023-12-03T17:37:06.981411Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_a (InputLayer)        [(None, 51, 1)]              0         []                            \n                                                                                                  \n input_b (InputLayer)        [(None, 51, 1)]              0         []                            \n                                                                                                  \n lstm (LSTM)                 (None, 64)                   16896     ['input_a[0][0]']             \n                                                                                                  \n lstm_1 (LSTM)               (None, 64)                   16896     ['input_b[0][0]']             \n                                                                                                  \n encoder_ab_output (Concate  (None, 128)                  0         ['lstm[0][0]',                \n nate)                                                               'lstm_1[0][0]']              \n                                                                                                  \n repeat_vector (RepeatVecto  (None, 51, 128)              0         ['encoder_ab_output[0][0]']   \n r)                                                                                               \n                                                                                                  \n lstm_2 (LSTM)               (None, 51, 51)               36720     ['repeat_vector[0][0]']       \n                                                                                                  \n time_distributed (TimeDist  (None, 51, 1)                52        ['lstm_2[0][0]']              \n ributed)                                                                                         \n                                                                                                  \n==================================================================================================\nTotal params: 70564 (275.64 KB)\nTrainable params: 70564 (275.64 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"tok_complex_list = tokenizer.texts_to_sequences(complex_list)\ntok_simpl_sent1_list = tokenizer.texts_to_sequences(simpl_sent1_list)\ntok_simpl_sent2_list = tokenizer.texts_to_sequences(simpl_sent2_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:37:06.985592Z","iopub.execute_input":"2023-12-03T17:37:06.985875Z","iopub.status.idle":"2023-12-03T17:38:16.170113Z","shell.execute_reply.started":"2023-12-03T17:37:06.985850Z","shell.execute_reply":"2023-12-03T17:38:16.169120Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tok_complex_list = pad_sequences(tok_complex_list, maxlen=max_tok_size)\ntok_simpl_sent1_list = pad_sequences(tok_simpl_sent1_list, maxlen=max_tok_size)\ntok_simpl_sent2_list = pad_sequences(tok_simpl_sent2_list, maxlen=max_tok_size)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:38:16.171220Z","iopub.execute_input":"2023-12-03T17:38:16.171510Z","iopub.status.idle":"2023-12-03T17:38:32.159859Z","shell.execute_reply.started":"2023-12-03T17:38:16.171485Z","shell.execute_reply":"2023-12-03T17:38:32.158867Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(type(tok_complex_list))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T17:38:32.162755Z","iopub.execute_input":"2023-12-03T17:38:32.163434Z","iopub.status.idle":"2023-12-03T17:38:32.168129Z","shell.execute_reply.started":"2023-12-03T17:38:32.163395Z","shell.execute_reply":"2023-12-03T17:38:32.167280Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(tokenizer.index_word))\nprint(len(tokenizer.index_word.keys()))\nprint([x/len(tokenizer.index_word) for x in tok_complex_list[0]])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:07:11.751235Z","iopub.execute_input":"2023-12-03T14:07:11.751571Z","iopub.status.idle":"2023-12-03T14:07:11.758451Z","shell.execute_reply.started":"2023-12-03T14:07:11.751539Z","shell.execute_reply":"2023-12-03T14:07:11.757398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok_max = len(tokenizer.index_word)\ntok_complex_list = tok_complex_list/tok_max\ntok_simpl_sent1_list = tok_simpl_sent1_list/tok_max\ntok_simpl_sent2_list = tok_simpl_sent2_list/tok_max","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:07:11.760527Z","iopub.execute_input":"2023-12-03T14:07:11.760770Z","iopub.status.idle":"2023-12-03T14:10:40.185215Z","shell.execute_reply.started":"2023-12-03T14:07:11.760750Z","shell.execute_reply":"2023-12-03T14:10:40.183909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok_complex_list = tok_complex_list.reshape(-1, tok_complex_list.shape[1], 1)\ntok_simpl_sent1_list = tok_simpl_sent1_list.reshape(-1, tok_simpl_sent1_list.shape[1], 1)\ntok_simpl_sent2_list = tok_simpl_sent2_list.reshape(-1, tok_simpl_sent2_list.shape[1], 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_per = int(.8 * len(tok_complex_list))\ntot = len(tok_complex_list)\ntok_complex_list_train, tok_complex_list_test = tok_complex_list[:val_per], tok_complex_list[val_per:]\ntok_simpl_sent1_list_train, tok_simpl_sent1_list_test = tok_simpl_sent1_list[:val_per], tok_simpl_sent1_list[val_per:]\ntok_simpl_sent2_list_train, tok_simpl_sent2_list_test = tok_simpl_sent2_list[:val_per], tok_simpl_sent2_list[val_per:]","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:10:40.186335Z","iopub.status.idle":"2023-12-03T14:10:40.186814Z","shell.execute_reply.started":"2023-12-03T14:10:40.186572Z","shell.execute_reply":"2023-12-03T14:10:40.186595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_model.fit([tok_simpl_sent1_list_train, tok_simpl_sent2_list_train], tok_complex_list_train, epochs=1, batch_size=32, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:10:40.188339Z","iopub.status.idle":"2023-12-03T14:10:40.188799Z","shell.execute_reply.started":"2023-12-03T14:10:40.188567Z","shell.execute_reply":"2023-12-03T14:10:40.188589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = siamese_model.predict([tok_simpl_sent1_list_test, tok_simpl_sent2_list_test])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:10:40.190803Z","iopub.status.idle":"2023-12-03T14:10:40.191153Z","shell.execute_reply.started":"2023-12-03T14:10:40.190984Z","shell.execute_reply":"2023-12-03T14:10:40.191001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(predictions.shape)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:10:40.192461Z","iopub.status.idle":"2023-12-03T14:10:40.192795Z","shell.execute_reply.started":"2023-12-03T14:10:40.192637Z","shell.execute_reply":"2023-12-03T14:10:40.192653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_model.save('merge_sentence.keras')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = preictions*tok_max","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = predictions.reshape(-1, predictions.shape[1])\npredictions = predictions.astype(numpy.int64)\noutput_texts = tokenizer.sequences_to_texts(predictions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dicty = { \"sent1\": simpl_sent1_list, \"sent2\": simpl_sent2_list, \"predict\": output_texts}\ndf_out = pd.DataFrame.from_dict(dicty)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_out.to_csv(\"submission.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}